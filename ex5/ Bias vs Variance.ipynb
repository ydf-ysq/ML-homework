{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c2ec62",
   "metadata": {},
   "source": [
    "# ex5 - Bias v.s.Variance\n",
    "在本次练习中，实现正则化线性回归，并使用它来研究具有不同偏差-方差特性的模型\n",
    "\n",
    "### Regularized Linear Regression\n",
    "在练习的前半部分，你将实现正则化线性回归，通过水库的水位变化来预测从水坝流出的水量。\n",
    "\n",
    "**Visualizing the dataset**\n",
    "$X$：水位变化的历史记录\n",
    "$y$：从水坝流出的水量\n",
    "这个数据集被分为三个部分：  \n",
    "- training set : 用于模型学习，$X, y$\n",
    "- cross validation set : 用于决定正则化参数，$Xval, yval$\n",
    "- test set : 用于评估性能，模型训练中是不可见的，$Xtest, ytest$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cfffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('ex5data1.mat')\n",
    "X = np.insert(data['X'], 0, 1, 1)\n",
    "y = data['y']\n",
    "Xval = np.insert(data['Xval'], 0, 1, 1)\n",
    "yval = data['yval']\n",
    "Xtest = np.insert(data['Xtest'], 0, 1, 1)\n",
    "ytest = data['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44174071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'water level': data['X'].ravel(), 'flow' : y.ravel()})\n",
    "df.plot.scatter(x = 'water level', y =  'flow', figsize = (12, 8),s = 50, marker = '+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604a529",
   "metadata": {},
   "source": [
    "**regularized cost function:**\n",
    "<img src = \"cost.jpg\" width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costReg(theta, X, y, Lambda):\n",
    "    theta = np.matrix(theta)\n",
    "#     print(X.shape, theta.shape)\n",
    "    inner = np.power(X @ theta.T - y, 2)\n",
    "    cost = np.sum(inner) /(2 * len(X))\n",
    "    reg = (Lambda / (2 * len(X))) * np.sum(np.power(theta[:, 1:], 2))\n",
    "    return cost + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c510aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.ones(X.shape[1])\n",
    "Lambda = 1\n",
    "costReg(theta, X, y, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cefc9",
   "metadata": {},
   "source": [
    "**regularized gradient:**\n",
    "<img src = \"grad.jpg\" width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, Lambda):    \n",
    "    theta = np.matrix(theta)\n",
    "    h = X @ theta.T\n",
    "    error = h - y\n",
    "    \n",
    "    reg = (Lambda / len(X)) * theta.T\n",
    "    reg[0] = 0\n",
    "    grad = X.T @ error / len(X) + reg\n",
    "    \n",
    "    return np.array(grad).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768625f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradientReg(theta, X, y, Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLinearReg(theta, X, y, Lambda):\n",
    "    res = opt.minimize(fun=costReg, x0=theta,\n",
    "                   args=(X, y, Lambda), method='TNC',\n",
    "                   jac=gradientReg,\n",
    "                   options={'disp': True})\n",
    "    return res\n",
    "\n",
    "res = trainLinearReg(theta, X, y, Lambda)\n",
    "theta_final = res['x']\n",
    "theta_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc88dec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotFit(theta, X, y):\n",
    "    h = X @ theta.T\n",
    "    x = X[:, 1:]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    ax.plot(x, y, '+', label = 'Training Data', markersize = 10)# 所给数据的散点图\n",
    "    ax.plot(x, h, '--', label = 'Prediction') # 最终确认的拟合直线\n",
    "    ax.legend()#图例，就是左上角那个说明块\n",
    "    plt.show()\n",
    "\n",
    "plotFit(theta_final, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb44a7f",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "学习曲线将训练和交叉验证误差绘制为训练集的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ac7c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotLearningRate(X, y, Xval, yval, Lambda):\n",
    "    m = X.shape[0]\n",
    "    error_train = np.zeros(m)\n",
    "    error_val = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        theta = np.ones(X.shape[1])\n",
    "        res = trainLinearReg(theta, X[:i + 1, :], y[:i + 1, :], Lambda)\n",
    "        error_train[i] = costReg(res['x'], X[:i + 1, :], y[:i + 1, :], 0)\n",
    "        error_val[i] = costReg(res['x'], Xval, yval, 0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.plot(np.arange(1, m + 1), error_train, label='Training cost')\n",
    "    ax.plot(np.arange(1, m + 1), error_val, label='Cross validation cost')\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "plotLearningRate(X, y, Xval, yval, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bb006",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "由上图可知，这个模型欠拟合了，因此我们需要添加更多的特征来解决这个问题。  \n",
    "使用多项式回归的假设函数如下：\n",
    "<img src = \"poly.jpg\" width = 520>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, power, as_ndarray = False):\n",
    "    X = X.ravel()\n",
    "    data = {'f{}'.format(i) : np.power(X, i) for i in range(1, power + 1)}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df.values if as_ndarray else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2170a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyFeatures(data['X'], power = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe49547",
   "metadata": {},
   "source": [
    "使用线性回归的代价函数来训练多项式回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49569b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize_(df):\n",
    "    return df.apply(lambda col: (col - col.mean()) / col.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a80109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparPolyData(*args, power):\n",
    "    def prepare(x):\n",
    "        df = polyFeatures(x, power)\n",
    "        ndarr = featureNormalize_(df).values\n",
    "        return np.insert(ndarr, 0, np.ones(ndarr.shape[0]), axis=1)\n",
    "    \n",
    "    return [prepare(x) for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fb383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def polyRegression(Lambda):\n",
    "    X_poly, Xval_poly, Xtest_poly = preparPolyData(data['X'], data['Xval'], data['Xtest'], power = 8)\n",
    "    theta1 = np.ones(X_poly.shape[1])\n",
    "    res = trainLinearReg(theta1, X_poly, y, Lambda)\n",
    "\n",
    "    # 多项式拟合图\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    x = X[:, 1:]\n",
    "    ax.plot(x, y, '+', label = 'Training Data', markersize = 10)# 所给数据的散点图\n",
    "\n",
    "    X_fit = np.linspace(x.min(), x.max(), 100)\n",
    "    X_fit_poly = preparPolyData(X_fit, power = 8)[0]\n",
    "    h = X_fit_poly @ res['x'].T\n",
    "    ax.plot(X_fit, h, '--', label = 'Prediction') # 最终确认的拟合直线\n",
    "    ax.legend()#图例，就是左上角那个说明块\n",
    "    plt.show()\n",
    "\n",
    "    # 多项式学习曲线\n",
    "    plotLearningRate(X_poly, y, Xval_poly, yval, Lambda)\n",
    "    \n",
    "polyRegression(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4688a",
   "metadata": {},
   "source": [
    "### Adjusting the regularization parameter \n",
    "从上图可知，当$\\lambda = 0$时，Training cost太低，是过拟合了。  \n",
    "因此，在这个部分，我们将观察$\\lambda$是如何影响正则化多项式回归的bias-variance的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyRegression(1) # Lambda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd79de2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "polyRegression(100) # Lambda = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce598aa",
   "metadata": {},
   "source": [
    "### Selecting $\\lambda$ using a cross validation set\n",
    "在这个部分，实现一个选择$\\lambda$的自动化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateLambda = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "# 使用多项式回归的特征\n",
    "def validationCurve(X, y, Xval, yval, Xtest, ytest, candidateLambda):\n",
    "    n = len(candidateLambda)\n",
    "    error_train = np.zeros(n)\n",
    "    error_val = np.zeros(n)    \n",
    "    error_test = np.zeros(n)    \n",
    "  \n",
    "    for i in range(n):\n",
    "        theta = np.ones(X.shape[1])\n",
    "        res = trainLinearReg(theta, X, y, candidateLambda[i])\n",
    "        error_train[i] = costReg(res['x'], X, y, 0)\n",
    "        error_val[i] = costReg(res['x'], Xval, yval, 0)\n",
    "        error_test[i] = costReg(res['x'], Xtest, ytest, 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.plot(candidateLambda, error_train, label='Training cost')\n",
    "    ax.plot(candidateLambda, error_val, label='Cross validation cost')\n",
    "    ax.plot(candidateLambda, error_test, label='Testing cost')\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    return error_train, error_val, error_test\n",
    "\n",
    "X_poly, Xval_poly, Xtest_poly = preparPolyData(data['X'], data['Xval'], data['Xtest'], power = 8)\n",
    "error_train, error_val, error_test = validationCurve(X_poly, y, Xval_poly, yval, Xtest_poly, ytest, candidateLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(candidateLambda)):\n",
    "    print('when Lambda = {}, test error={}.'.format(candidateLambda[i],error_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99b725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
